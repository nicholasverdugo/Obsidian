# Implementation of a Linear Regression Model
- #Linear-Regression implemented in code
- Dataset: {(xi, ti)}^n, i=1
- Regression task 
	- ti E R (labels)
	- xi E R (inputs)
- Feature Space: 
	- phi : R -> R^M
	- x->  (x^1, x^2, x^3, xxx, x^m)
	- m = hyperparameter
- Mapper function (or model):
	- f(phi(x), w)=w0 + w1 * x^1 + w2 * x^2 + ... + wm * x^m
	- SUM(wj * x^j), LIM=M, j=0
	- w = learnable unkown parameters
- #Objective-Function : typically a measure of the empirical error for #Regression 
	- e.g. 1/N * SUM(error_i)^2, LIM = N, i=1
	- also 1/N * SUM(t_i-y_i)^2, LIM = N, i=1
- Pseudocode for #Linear-Regression with polynomial features:
INPUT: 
input data {xi}^N, i=1
target values {ti}^N, i=1
hyperparameter M
```
Compute feature matrix X
Compute Solution for w=(XT * X)^-1 * XT *t
Predict labels for input data: y = X*w
return w, y
```
- The code built in today's lecture can run Short Assignment 1
- Apply ln to both sides of the output to get desired results
- since we want t = y,  ln(t)= SUM(wj x^j), LIM = M, j=0
- w = (XT X)^-1 * XT * ln(t)
- 