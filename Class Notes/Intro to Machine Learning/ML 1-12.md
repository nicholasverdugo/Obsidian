# Introduction to Supervised Learning
## Classification vs Regression
- #Classification : 0 or 1 - data can be placed in either one or another class, that's it!
- #Regression - a form of _predictive modeling_ approach to characterize the relationship between some collection of observational input data and a continuous desired response.
- Classifiers can be sub-categorized as well
	- #Discriminative: trying to find a partition - separating the feature space into two regions (anything above a line is one class, anything below the line is another class)
	- #Generative: When we have a test point, we can compute the posterior probability of that point belonging to each class and assign the point to the class with the highest posterior probability

## Supervised Learning Flowchart
- Training -> Testing -> Validating (?)

### Training
1. Collect labeled training data
	1. Often the most time consuming and expensive task 
	2. This constitutes the #input-space
2. Extract features
	1. Extract *useful* features from the input (or observational) data so that they have discriminatory information in mapping the output successfully. 
	2. This constitutes the #feature-space
3. Select a model
	1. This decides the relationship between input data and desired output
4. Fit the model
	1. Change the model parameters (**Learning Algorithm**) to meet some Objective Function

- You can use the derivative (gradient) of the #MSE(mean squared error) in order to find the minimum ( #gradient-descent )
	- This is done because we want to minimize error (obviously)

## Linear Regression
- #Linear-Regression is a supervised learning task (training data *must* contain **labels**)